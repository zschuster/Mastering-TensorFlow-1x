{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#MNIST-Dataset\" data-toc-modified-id=\"MNIST-Dataset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>MNIST Dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-the-MNIST-data\" data-toc-modified-id=\"Get-the-MNIST-data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Get the MNIST data</a></span></li><li><span><a href=\"#MLP-in-TensorFlow\" data-toc-modified-id=\"MLP-in-TensorFlow-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>MLP in TensorFlow</a></span></li><li><span><a href=\"#MLP-in-Keras\" data-toc-modified-id=\"MLP-in-Keras-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>MLP in Keras</a></span></li><li><span><a href=\"#MLP-in-TFLearn\" data-toc-modified-id=\"MLP-in-TFLearn-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>MLP in TFLearn</a></span></li></ul></li><li><span><a href=\"#TimeSeries-Data---MLP---Keras\" data-toc-modified-id=\"TimeSeries-Data---MLP---Keras-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>TimeSeries Data - MLP - Keras</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prepare-the-data\" data-toc-modified-id=\"Prepare-the-data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Prepare the data</a></span></li><li><span><a href=\"#Build,-Train-and-Evaluate-the-Model\" data-toc-modified-id=\"Build,-Train-and-Evaluate-the-Model-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Build, Train and Evaluate the Model</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLayer Perceptron <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:1.15.4\n",
      "Pandas:0.24.0\n",
      "Matplotlib:3.0.2\n",
      "TensorFlow:1.12.0\n",
      "Keras:2.2.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "print(\"NumPy:{}\".format(np.__version__))\n",
    "\n",
    "import pandas as pd\n",
    "print(\"Pandas:{}\".format(pd.__version__))\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = [15, 10]\n",
    "print(\"Matplotlib:{}\".format(mpl.__version__))\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(123)\n",
    "print(\"TensorFlow:{}\".format(tf.__version__))\n",
    "\n",
    "import keras\n",
    "print(\"Keras:{}\".format(keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASETSLIB_HOME = '../datasetslib'\n",
    "# import sys\n",
    "# if not DATASETSLIB_HOME in sys.path:\n",
    "#     sys.path.append(DATASETSLIB_HOME)\n",
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "# import datasetslib\n",
    "\n",
    "# from datasetslib import util as dsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasetslib.datasets_root = os.path.join(os.path.expanduser('~'),'datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(os.path.join(datasetslib.datasets_root, 'mnist'),\n",
    "#                                   one_hot=True)\n",
    "\n",
    "# X_train = mnist.train.images\n",
    "# X_test = mnist.test.images\n",
    "# Y_train = mnist.train.labels\n",
    "# Y_test = mnist.test.labels\n",
    "\n",
    "# num_outputs = 10  # 0-9 digits\n",
    "# num_inputs = 784  # total pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the keras modules\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# define some hyper parameters\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "# get the data\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# reshape the two dimensional 28 x 28 pixels\n",
    "#   sized images into a single vector of 784 pixels\n",
    "X_train = X_train.reshape(60000, num_inputs)\n",
    "X_test = X_test.reshape(10000, num_inputs)\n",
    "\n",
    "# convert the input values to float32\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "# normalize the values of image vectors to fit under 1\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert output data into one hot encoded format\n",
    "Y_train = keras.utils.to_categorical(Y_train, num_outputs)\n",
    "Y_test = keras.utils.to_categorical(Y_test, num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, num_inputs, num_outputs, num_layers, num_neurons):\n",
    "    w = []\n",
    "    b = []\n",
    "    for i in range(num_layers):\n",
    "        # weights\n",
    "        w.append(tf.Variable(tf.random_normal(\n",
    "            [num_inputs if i == 0 else num_neurons[i - 1],\n",
    "             num_neurons[i]]),\n",
    "            name=\"w_{0:04d}\".format(i)\n",
    "        ))\n",
    "        # biases\n",
    "        b.append(tf.Variable(tf.random_normal(\n",
    "            [num_neurons[i]]),\n",
    "            name=\"b_{0:04d}\".format(i)\n",
    "        ))\n",
    "    w.append(tf.Variable(tf.random_normal(\n",
    "        [num_neurons[num_layers - 1] if num_layers > 0 else num_inputs,\n",
    "         num_outputs]), name=\"w_out\"))\n",
    "    b.append(tf.Variable(tf.random_normal([num_outputs]), name=\"b_out\"))\n",
    "\n",
    "    # x is input layer\n",
    "    layer = x\n",
    "    # add hidden layers\n",
    "    for i in range(num_layers):\n",
    "        layer = tf.nn.relu(tf.matmul(layer, w[i]) + b[i])\n",
    "    # add output layer\n",
    "    layer = tf.matmul(layer, w[num_layers]) + b[num_layers]\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "# def mnist_batch_func(batch_size=100):\n",
    "#     X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "#     return [X_batch, Y_batch]\n",
    "\n",
    "def mnist_batch_func(X = X_train, Y = Y_train, batch_size=100):\n",
    "    \n",
    "    n_batches = X.shape[0] / batch_size\n",
    "    \n",
    "    X_batch_list = np.array_split(X, n_batches)\n",
    "    Y_batch_list = np.array_split(Y, n_batches)\n",
    "    \n",
    "#     X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "    return [X_batch_list, Y_batch_list]\n",
    "\n",
    "\n",
    "def tensorflow_classification(n_epochs, n_batches,\n",
    "                              batch_size, batch_func,\n",
    "                              model, optimizer, loss, accuracy_function,\n",
    "                              X_test, Y_test):\n",
    "    with tf.Session() as tfs:\n",
    "        tfs.run(tf.global_variables_initializer())\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            X_batch_list, Y_batch_list = mnist_batch_func(batch_size = batch_size)\n",
    "            for batch in range(n_batches):\n",
    "                X_batch, Y_batch = X_batch_list[batch], Y_batch_list[batch]\n",
    "                feed_dict = {x: X_batch, y: Y_batch}\n",
    "                _, batch_loss = tfs.run([optimizer, loss], feed_dict)\n",
    "                epoch_loss += batch_loss\n",
    "            average_loss = epoch_loss / n_batches\n",
    "            print(\"epoch: {0:04d}   loss = {1:0.6f}\".format(\n",
    "                epoch, average_loss))\n",
    "        feed_dict = {x: X_test, y: Y_test}\n",
    "        accuracy_score = tfs.run(accuracy_function, feed_dict=feed_dict)\n",
    "        print(\"accuracy={0:.8f}\".format(accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y = mnist_batch_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output length of each batch to make sure it makes sense\n",
    "# for i in range(len(X)):\n",
    "#     print((X[i].shape[0], Y[i].shape[0]), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"random_normal:0\", shape=(4, 5), dtype=float32)\n",
      "output :\n",
      " [[ 1.725308    2.0050526  -0.49619502  0.91051316  0.58666545]\n",
      " [ 0.45031202  0.8317719   0.918999   -0.8434593   0.221711  ]\n",
      " [-2.4935062   0.49829382 -0.37255976 -0.13000494 -1.2136227 ]\n",
      " [ 1.2458911   0.7678803  -0.36117166 -0.71996236 -0.57993555]]\n"
     ]
    }
   ],
   "source": [
    "# example of the tf.random_normal function\n",
    "\n",
    "x = tf.random_normal([4, 5])\n",
    "print(x)\n",
    "\n",
    "with tf.Session() as tfs:\n",
    "    # initialize and print the variable y\n",
    "    tf.global_variables_initializer().run()\n",
    "    output = tfs.run(tf.random_normal([4, 5]))\n",
    "print('output :\\n', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0000   loss = 1.311476\n",
      "epoch: 0001   loss = 0.530669\n",
      "epoch: 0002   loss = 0.439899\n",
      "epoch: 0003   loss = 0.395943\n",
      "epoch: 0004   loss = 0.369433\n",
      "epoch: 0005   loss = 0.351642\n",
      "epoch: 0006   loss = 0.338863\n",
      "epoch: 0007   loss = 0.329241\n",
      "epoch: 0008   loss = 0.321765\n",
      "epoch: 0009   loss = 0.315803\n",
      "epoch: 0010   loss = 0.310942\n",
      "epoch: 0011   loss = 0.306913\n",
      "epoch: 0012   loss = 0.303530\n",
      "epoch: 0013   loss = 0.300659\n",
      "epoch: 0014   loss = 0.298199\n",
      "epoch: 0015   loss = 0.296071\n",
      "epoch: 0016   loss = 0.294214\n",
      "epoch: 0017   loss = 0.292578\n",
      "epoch: 0018   loss = 0.291127\n",
      "epoch: 0019   loss = 0.289832\n",
      "epoch: 0020   loss = 0.288669\n",
      "epoch: 0021   loss = 0.287621\n",
      "epoch: 0022   loss = 0.286674\n",
      "epoch: 0023   loss = 0.285818\n",
      "epoch: 0024   loss = 0.285044\n",
      "epoch: 0025   loss = 0.284339\n",
      "epoch: 0026   loss = 0.283692\n",
      "epoch: 0027   loss = 0.283094\n",
      "epoch: 0028   loss = 0.282538\n",
      "epoch: 0029   loss = 0.282019\n",
      "epoch: 0030   loss = 0.281534\n",
      "epoch: 0031   loss = 0.281079\n",
      "epoch: 0032   loss = 0.280653\n",
      "epoch: 0033   loss = 0.280252\n",
      "epoch: 0034   loss = 0.279874\n",
      "epoch: 0035   loss = 0.279519\n",
      "epoch: 0036   loss = 0.279184\n",
      "epoch: 0037   loss = 0.278869\n",
      "epoch: 0038   loss = 0.278571\n",
      "epoch: 0039   loss = 0.278291\n",
      "epoch: 0040   loss = 0.278026\n",
      "epoch: 0041   loss = 0.277776\n",
      "epoch: 0042   loss = 0.277540\n",
      "epoch: 0043   loss = 0.277317\n",
      "epoch: 0044   loss = 0.277106\n",
      "epoch: 0045   loss = 0.276906\n",
      "epoch: 0046   loss = 0.276716\n",
      "epoch: 0047   loss = 0.276536\n",
      "epoch: 0048   loss = 0.276365\n",
      "epoch: 0049   loss = 0.276202\n",
      "accuracy=0.90090001\n"
     ]
    }
   ],
   "source": [
    "num_layers = 0\n",
    "num_neurons = []\n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "n_batches = int(X_train.shape[0] / batch_size)#int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "# input images\n",
    "x = tf.placeholder(dtype=tf.float32, name=\"x\", shape=[None, num_inputs])\n",
    "# target output\n",
    "y = tf.placeholder(dtype=tf.float32, name=\"y\", shape=[None, num_outputs])\n",
    "\n",
    "model = mlp(x=x,\n",
    "            num_inputs=num_inputs,\n",
    "            num_outputs=num_outputs,\n",
    "            num_layers=num_layers,\n",
    "            num_neurons=num_neurons)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=y))\n",
    "# optimizer function\n",
    "optimizer = tf.train.AdamOptimizer( # try with adam optimizer instead of cross entropy. accuracy was about 86% with cross ent\n",
    "    learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "predictions_check = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "accuracy_function = tf.reduce_mean(tf.cast(predictions_check, tf.float32))\n",
    "\n",
    "tensorflow_classification(n_epochs=n_epochs,\n",
    "                          n_batches=n_batches,\n",
    "                          batch_size=batch_size,\n",
    "                          batch_func=mnist_batch_func,\n",
    "                          model=model,\n",
    "                          optimizer=optimizer,\n",
    "                          loss=loss,\n",
    "                          accuracy_function=accuracy_function,\n",
    "                          X_test=X_test,\n",
    "                          Y_test=Y_test\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I've now seen multiple times that the Adam (Spannbauer?) optimizer outperforms SGD and GD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-b6a4bbb30cef>:24: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "epoch: 0000   loss = 4.501620\n",
      "epoch: 0001   loss = 2.165700\n",
      "epoch: 0002   loss = 1.962173\n",
      "epoch: 0003   loss = 1.836360\n",
      "epoch: 0004   loss = 1.742084\n",
      "epoch: 0005   loss = 1.665271\n",
      "epoch: 0006   loss = 1.601830\n",
      "epoch: 0007   loss = 1.548200\n",
      "epoch: 0008   loss = 1.501288\n",
      "epoch: 0009   loss = 1.458790\n",
      "epoch: 0010   loss = 1.420701\n",
      "epoch: 0011   loss = 1.385863\n",
      "epoch: 0012   loss = 1.353231\n",
      "epoch: 0013   loss = 1.322490\n",
      "epoch: 0014   loss = 1.293384\n",
      "epoch: 0015   loss = 1.265378\n",
      "epoch: 0016   loss = 1.238238\n",
      "epoch: 0017   loss = 1.212012\n",
      "epoch: 0018   loss = 1.186574\n",
      "epoch: 0019   loss = 1.161732\n",
      "epoch: 0020   loss = 1.137507\n",
      "epoch: 0021   loss = 1.113608\n",
      "epoch: 0022   loss = 1.090136\n",
      "epoch: 0023   loss = 1.067153\n",
      "epoch: 0024   loss = 1.044782\n",
      "epoch: 0025   loss = 1.023373\n",
      "epoch: 0026   loss = 1.002885\n",
      "epoch: 0027   loss = 0.983813\n",
      "epoch: 0028   loss = 0.966016\n",
      "epoch: 0029   loss = 0.949272\n",
      "epoch: 0030   loss = 0.933522\n",
      "epoch: 0031   loss = 0.918646\n",
      "epoch: 0032   loss = 0.904685\n",
      "epoch: 0033   loss = 0.891414\n",
      "epoch: 0034   loss = 0.878960\n",
      "epoch: 0035   loss = 0.867413\n",
      "epoch: 0036   loss = 0.856599\n",
      "epoch: 0037   loss = 0.846379\n",
      "epoch: 0038   loss = 0.836629\n",
      "epoch: 0039   loss = 0.827418\n",
      "epoch: 0040   loss = 0.818742\n",
      "epoch: 0041   loss = 0.810533\n",
      "epoch: 0042   loss = 0.802634\n",
      "epoch: 0043   loss = 0.795035\n",
      "epoch: 0044   loss = 0.787814\n",
      "epoch: 0045   loss = 0.780921\n",
      "epoch: 0046   loss = 0.774344\n",
      "epoch: 0047   loss = 0.768048\n",
      "epoch: 0048   loss = 0.762012\n",
      "epoch: 0049   loss = 0.756186\n",
      "accuracy=0.74710000\n"
     ]
    }
   ],
   "source": [
    "num_layers = 1 \n",
    "num_neurons = []\n",
    "for i in range(num_layers):\n",
    "    num_neurons.append(8)\n",
    "    \n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "n_batches = int(X_train.shape[0] / batch_size)#int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "# input images\n",
    "x = tf.placeholder(dtype=tf.float32, name=\"x\", shape=[None, num_inputs]) \n",
    "# target output\n",
    "y = tf.placeholder(dtype=tf.float32, name=\"y\", shape=[None, num_outputs]) \n",
    "\n",
    "model = mlp(x=x, \n",
    "            num_inputs=num_inputs, \n",
    "            num_outputs=num_outputs, \n",
    "            num_layers=num_layers, \n",
    "            num_neurons=num_neurons)\n",
    "\n",
    "# loss function\n",
    "#loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(model), axis=1))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y))\n",
    "# optimizer function\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "predictions_check = tf.equal(tf.argmax(model,1), tf.argmax(y,1))\n",
    "accuracy_function = tf.reduce_mean(tf.cast(predictions_check, tf.float32))\n",
    "\n",
    "tensorflow_classification(n_epochs=n_epochs, \n",
    "                          n_batches=n_batches, \n",
    "                          batch_size=batch_size, \n",
    "                          batch_func=mnist_batch_func, \n",
    "                          model = model, \n",
    "                          optimizer = optimizer, \n",
    "                          loss = loss, \n",
    "                          accuracy_function = accuracy_function, \n",
    "                          X_test = X_test, \n",
    "                          Y_test = Y_test\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0000   loss = 47.227662\n",
      "epoch: 0001   loss = 8.538160\n",
      "epoch: 0002   loss = 4.646583\n",
      "epoch: 0003   loss = 3.348860\n",
      "epoch: 0004   loss = 2.793539\n",
      "epoch: 0005   loss = 2.250838\n",
      "epoch: 0006   loss = 2.194859\n",
      "epoch: 0007   loss = 1.737916\n",
      "epoch: 0008   loss = 1.654112\n",
      "epoch: 0009   loss = 1.292771\n",
      "epoch: 0010   loss = 1.471043\n",
      "epoch: 0011   loss = 1.166706\n",
      "epoch: 0012   loss = 1.087869\n",
      "epoch: 0013   loss = 0.930347\n",
      "epoch: 0014   loss = 0.693916\n",
      "epoch: 0015   loss = 0.726475\n",
      "epoch: 0016   loss = 0.622864\n",
      "epoch: 0017   loss = 0.704298\n",
      "epoch: 0018   loss = 0.572432\n",
      "epoch: 0019   loss = 0.406954\n",
      "epoch: 0020   loss = 0.418747\n",
      "epoch: 0021   loss = 0.421859\n",
      "epoch: 0022   loss = 0.397260\n",
      "epoch: 0023   loss = 0.250440\n",
      "epoch: 0024   loss = 0.302062\n",
      "epoch: 0025   loss = 0.245605\n",
      "epoch: 0026   loss = 0.257510\n",
      "epoch: 0027   loss = 0.187336\n",
      "epoch: 0028   loss = 0.196303\n",
      "epoch: 0029   loss = 0.181649\n",
      "epoch: 0030   loss = 0.197445\n",
      "epoch: 0031   loss = 0.187419\n",
      "epoch: 0032   loss = 0.174503\n",
      "epoch: 0033   loss = 0.149910\n",
      "epoch: 0034   loss = 0.171837\n",
      "epoch: 0035   loss = 0.116349\n",
      "epoch: 0036   loss = 0.110332\n",
      "epoch: 0037   loss = 0.167296\n",
      "epoch: 0038   loss = 0.134243\n",
      "epoch: 0039   loss = 0.126671\n",
      "epoch: 0040   loss = 0.099655\n",
      "epoch: 0041   loss = 0.111638\n",
      "epoch: 0042   loss = 0.134346\n",
      "epoch: 0043   loss = 0.108940\n",
      "epoch: 0044   loss = 0.093897\n",
      "epoch: 0045   loss = 0.119104\n",
      "epoch: 0046   loss = 0.121298\n",
      "epoch: 0047   loss = 0.079374\n",
      "epoch: 0048   loss = 0.089597\n",
      "epoch: 0049   loss = 0.133886\n",
      "accuracy=0.96410000\n"
     ]
    }
   ],
   "source": [
    "num_layers = 2\n",
    "num_neurons = []\n",
    "for i in range(num_layers):\n",
    "    num_neurons.append(256)\n",
    "    \n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "n_batches = int(X_train.shape[0] / batch_size) # int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "# input images\n",
    "x = tf.placeholder(dtype=tf.float32, name=\"x\", shape=[None, num_inputs]) \n",
    "# target output\n",
    "y = tf.placeholder(dtype=tf.float32, name=\"y\", shape=[None, num_outputs]) \n",
    "\n",
    "model = mlp(x=x, \n",
    "            num_inputs=num_inputs, \n",
    "            num_outputs=num_outputs, \n",
    "            num_layers=num_layers, \n",
    "            num_neurons=num_neurons)\n",
    "\n",
    "# loss function\n",
    "#loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(model), axis=1))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, \n",
    "                                                              labels=y))\n",
    "# optimizer function\n",
    "# try with adam optimizer instead of cross entropy. accuracy was about 92.7% with cross ent\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss) \n",
    "\n",
    "predictions_check = tf.equal(tf.argmax(model,1), tf.argmax(y,1))\n",
    "accuracy_function = tf.reduce_mean(tf.cast(predictions_check, tf.float32))\n",
    "\n",
    "tensorflow_classification(n_epochs=n_epochs, \n",
    "                          n_batches=n_batches, \n",
    "                          batch_size=batch_size, \n",
    "                          batch_func=mnist_batch_func, \n",
    "                          model = model, \n",
    "                          optimizer = optimizer, \n",
    "                          loss = loss, \n",
    "                          accuracy_function = accuracy_function, \n",
    "                          X_test = X_test, \n",
    "                          Y_test = Y_test\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We get 96% accuracy with the Adam optimizer which is a gain of 3.4 percentage points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0000   loss = 49702.096204\n",
      "epoch: 0001   loss = 3030.654102\n",
      "epoch: 0002   loss = 1534.149350\n",
      "epoch: 0003   loss = 963.586777\n",
      "epoch: 0004   loss = 616.895087\n",
      "epoch: 0005   loss = 528.032386\n",
      "epoch: 0006   loss = 438.094556\n",
      "epoch: 0007   loss = 400.286694\n",
      "epoch: 0008   loss = 311.637628\n",
      "epoch: 0009   loss = 312.973464\n",
      "epoch: 0010   loss = 246.258202\n",
      "epoch: 0011   loss = 243.268431\n",
      "epoch: 0012   loss = 223.890757\n",
      "epoch: 0013   loss = 196.757697\n",
      "epoch: 0014   loss = 165.561026\n",
      "epoch: 0015   loss = 130.100145\n",
      "epoch: 0016   loss = 99.514384\n",
      "epoch: 0017   loss = 94.655479\n",
      "epoch: 0018   loss = 71.194322\n",
      "epoch: 0019   loss = 64.339540\n",
      "epoch: 0020   loss = 54.822529\n",
      "epoch: 0021   loss = 45.478319\n",
      "epoch: 0022   loss = 37.880638\n",
      "epoch: 0023   loss = 26.872124\n",
      "epoch: 0024   loss = 24.782240\n",
      "epoch: 0025   loss = 24.490945\n",
      "epoch: 0026   loss = 15.259554\n",
      "epoch: 0027   loss = 10.733762\n",
      "epoch: 0028   loss = 11.014392\n",
      "epoch: 0029   loss = 10.792732\n",
      "epoch: 0030   loss = 5.529969\n",
      "epoch: 0031   loss = 2.846322\n",
      "epoch: 0032   loss = 1.889759\n",
      "epoch: 0033   loss = 1.759659\n",
      "epoch: 0034   loss = 1.824596\n",
      "epoch: 0035   loss = 2.366991\n",
      "epoch: 0036   loss = 0.987274\n",
      "epoch: 0037   loss = 0.641882\n",
      "epoch: 0038   loss = 0.646403\n",
      "epoch: 0039   loss = 2.869404\n",
      "epoch: 0040   loss = 1.999257\n",
      "epoch: 0041   loss = 1.947578\n",
      "epoch: 0042   loss = 1.937009\n",
      "epoch: 0043   loss = 2.131251\n",
      "epoch: 0044   loss = 2.290974\n",
      "epoch: 0045   loss = 2.211763\n",
      "epoch: 0046   loss = 2.252196\n",
      "epoch: 0047   loss = 2.235966\n",
      "epoch: 0048   loss = 2.272221\n",
      "epoch: 0049   loss = 2.302990\n",
      "epoch: 0050   loss = 2.301870\n",
      "epoch: 0051   loss = 2.301875\n",
      "epoch: 0052   loss = 2.301879\n",
      "epoch: 0053   loss = 2.301882\n",
      "epoch: 0054   loss = 2.301884\n",
      "epoch: 0055   loss = 2.301885\n",
      "epoch: 0056   loss = 2.301887\n",
      "epoch: 0057   loss = 2.301887\n",
      "epoch: 0058   loss = 2.301888\n",
      "epoch: 0059   loss = 2.301889\n",
      "epoch: 0060   loss = 2.301889\n",
      "epoch: 0061   loss = 2.301889\n",
      "epoch: 0062   loss = 2.301889\n",
      "epoch: 0063   loss = 2.301890\n",
      "epoch: 0064   loss = 2.301890\n",
      "epoch: 0065   loss = 2.301890\n",
      "epoch: 0066   loss = 2.301890\n",
      "epoch: 0067   loss = 2.301890\n",
      "epoch: 0068   loss = 2.301890\n",
      "epoch: 0069   loss = 2.301890\n",
      "epoch: 0070   loss = 2.301890\n",
      "epoch: 0071   loss = 2.301890\n",
      "epoch: 0072   loss = 2.301890\n",
      "epoch: 0073   loss = 2.301890\n",
      "epoch: 0074   loss = 2.301890\n",
      "epoch: 0075   loss = 2.301890\n",
      "epoch: 0076   loss = 2.301890\n",
      "epoch: 0077   loss = 2.301890\n",
      "epoch: 0078   loss = 2.301890\n",
      "epoch: 0079   loss = 2.301890\n",
      "epoch: 0080   loss = 2.301890\n",
      "epoch: 0081   loss = 2.301890\n",
      "epoch: 0082   loss = 2.301890\n",
      "epoch: 0083   loss = 2.301890\n",
      "epoch: 0084   loss = 2.301890\n",
      "epoch: 0085   loss = 2.301890\n",
      "epoch: 0086   loss = 2.301890\n",
      "epoch: 0087   loss = 2.301890\n",
      "epoch: 0088   loss = 2.301890\n",
      "epoch: 0089   loss = 2.301890\n",
      "epoch: 0090   loss = 2.301890\n",
      "epoch: 0091   loss = 2.301890\n",
      "epoch: 0092   loss = 2.301890\n",
      "epoch: 0093   loss = 2.301890\n",
      "epoch: 0094   loss = 2.301890\n",
      "epoch: 0095   loss = 2.301890\n",
      "epoch: 0096   loss = 2.301890\n",
      "epoch: 0097   loss = 2.301890\n",
      "epoch: 0098   loss = 2.301890\n",
      "epoch: 0099   loss = 2.301890\n",
      "accuracy=0.10280000\n"
     ]
    }
   ],
   "source": [
    "num_layers = 5 # Go big or go home, right?\n",
    "num_neurons = []\n",
    "for i in range(num_layers):\n",
    "    num_neurons.append(256)\n",
    "    \n",
    "learning_rate = 0.02 # Increase the learning rate by .01\n",
    "n_epochs = 100 # double number of epochs\n",
    "batch_size = 200 # double batch size\n",
    "n_batches = int(X_train.shape[0] / batch_size) # int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "# input images\n",
    "x = tf.placeholder(dtype=tf.float32, name=\"x\", shape=[None, num_inputs]) \n",
    "# target output\n",
    "y = tf.placeholder(dtype=tf.float32, name=\"y\", shape=[None, num_outputs]) \n",
    "\n",
    "model = mlp(x=x, \n",
    "            num_inputs=num_inputs, \n",
    "            num_outputs=num_outputs, \n",
    "            num_layers=num_layers, \n",
    "            num_neurons=num_neurons)\n",
    "\n",
    "# loss function\n",
    "#loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(model), axis=1))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, \n",
    "                                                              labels=y))\n",
    "# optimizer function\n",
    "# try with adam optimizer instead of cross entropy. accuracy was about 92.7% with cross ent\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss) \n",
    "\n",
    "predictions_check = tf.equal(tf.argmax(model,1), tf.argmax(y,1))\n",
    "accuracy_function = tf.reduce_mean(tf.cast(predictions_check, tf.float32))\n",
    "\n",
    "tensorflow_classification(n_epochs=n_epochs, \n",
    "                          n_batches=n_batches, \n",
    "                          batch_size=batch_size, \n",
    "                          batch_func=mnist_batch_func, \n",
    "                          model = model, \n",
    "                          optimizer = optimizer, \n",
    "                          loss = loss, \n",
    "                          accuracy_function = accuracy_function, \n",
    "                          X_test = X_test, \n",
    "                          Y_test = Y_test\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My best accuracy with 5 layers and 256 nuerons/layer was about 12.3%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0000   loss = 515.546812\n",
      "epoch: 0001   loss = 55.843381\n",
      "epoch: 0002   loss = 28.693286\n",
      "epoch: 0003   loss = 20.870644\n",
      "epoch: 0004   loss = 12.963480\n",
      "epoch: 0005   loss = 10.858646\n",
      "epoch: 0006   loss = 9.496291\n",
      "epoch: 0007   loss = 7.863766\n",
      "epoch: 0008   loss = 8.113810\n",
      "epoch: 0009   loss = 7.085378\n",
      "epoch: 0010   loss = 6.299392\n",
      "epoch: 0011   loss = 5.946728\n",
      "epoch: 0012   loss = 4.767405\n",
      "epoch: 0013   loss = 3.436809\n",
      "epoch: 0014   loss = 2.901877\n",
      "epoch: 0015   loss = 1.941250\n",
      "epoch: 0016   loss = 2.025750\n",
      "epoch: 0017   loss = 2.228022\n",
      "epoch: 0018   loss = 1.245127\n",
      "epoch: 0019   loss = 0.646400\n",
      "epoch: 0020   loss = 0.644938\n",
      "epoch: 0021   loss = 0.496506\n",
      "epoch: 0022   loss = 0.419017\n",
      "epoch: 0023   loss = 0.453017\n",
      "epoch: 0024   loss = 0.400648\n",
      "epoch: 0025   loss = 0.277305\n",
      "epoch: 0026   loss = 0.262131\n",
      "epoch: 0027   loss = 0.397849\n",
      "epoch: 0028   loss = 0.298090\n",
      "epoch: 0029   loss = 0.223871\n",
      "epoch: 0030   loss = 0.243258\n",
      "epoch: 0031   loss = 0.182572\n",
      "epoch: 0032   loss = 0.270389\n",
      "epoch: 0033   loss = 0.645940\n",
      "epoch: 0034   loss = 2.035282\n",
      "epoch: 0035   loss = 2.283869\n",
      "epoch: 0036   loss = 2.262459\n",
      "epoch: 0037   loss = 2.279190\n",
      "epoch: 0038   loss = 2.267758\n",
      "epoch: 0039   loss = 2.296170\n",
      "epoch: 0040   loss = 2.279713\n",
      "epoch: 0041   loss = 2.317621\n",
      "epoch: 0042   loss = 2.301785\n",
      "epoch: 0043   loss = 2.301784\n",
      "epoch: 0044   loss = 2.301783\n",
      "epoch: 0045   loss = 2.301782\n",
      "epoch: 0046   loss = 2.301782\n",
      "epoch: 0047   loss = 2.301781\n",
      "epoch: 0048   loss = 2.301781\n",
      "epoch: 0049   loss = 2.301781\n",
      "epoch: 0050   loss = 2.301781\n",
      "epoch: 0051   loss = 2.301781\n",
      "epoch: 0052   loss = 2.301780\n",
      "epoch: 0053   loss = 2.301780\n",
      "epoch: 0054   loss = 2.301780\n",
      "epoch: 0055   loss = 2.301780\n",
      "epoch: 0056   loss = 2.301780\n",
      "epoch: 0057   loss = 2.301780\n",
      "epoch: 0058   loss = 2.301780\n",
      "epoch: 0059   loss = 2.301780\n",
      "epoch: 0060   loss = 2.301780\n",
      "epoch: 0061   loss = 2.301780\n",
      "epoch: 0062   loss = 2.301780\n",
      "epoch: 0063   loss = 2.301780\n",
      "epoch: 0064   loss = 2.301780\n",
      "epoch: 0065   loss = 2.301780\n",
      "epoch: 0066   loss = 2.301780\n",
      "epoch: 0067   loss = 2.301780\n",
      "epoch: 0068   loss = 2.301780\n",
      "epoch: 0069   loss = 2.301780\n",
      "epoch: 0070   loss = 2.301780\n",
      "epoch: 0071   loss = 2.301780\n",
      "epoch: 0072   loss = 2.301780\n",
      "epoch: 0073   loss = 2.301780\n",
      "epoch: 0074   loss = 2.301780\n",
      "epoch: 0075   loss = 2.301780\n",
      "epoch: 0076   loss = 2.301780\n",
      "epoch: 0077   loss = 2.301780\n",
      "epoch: 0078   loss = 2.301780\n",
      "epoch: 0079   loss = 2.301780\n",
      "epoch: 0080   loss = 2.301780\n",
      "epoch: 0081   loss = 2.301780\n",
      "epoch: 0082   loss = 2.301780\n",
      "epoch: 0083   loss = 2.301780\n",
      "epoch: 0084   loss = 2.301780\n",
      "epoch: 0085   loss = 2.301780\n",
      "epoch: 0086   loss = 2.301780\n",
      "epoch: 0087   loss = 2.301780\n",
      "epoch: 0088   loss = 2.301780\n",
      "epoch: 0089   loss = 2.301780\n",
      "epoch: 0090   loss = 2.301780\n",
      "epoch: 0091   loss = 2.301780\n",
      "epoch: 0092   loss = 2.301780\n",
      "epoch: 0093   loss = 2.301780\n",
      "epoch: 0094   loss = 2.301780\n",
      "epoch: 0095   loss = 2.301780\n",
      "epoch: 0096   loss = 2.301780\n",
      "epoch: 0097   loss = 2.301780\n",
      "epoch: 0098   loss = 2.301780\n",
      "epoch: 0099   loss = 2.301780\n",
      "accuracy=0.10280000\n"
     ]
    }
   ],
   "source": [
    "num_layers = 3 \n",
    "num_neurons = []\n",
    "for i in range(num_layers):\n",
    "    num_neurons.append(256)\n",
    "    \n",
    "learning_rate = 0.02 # Increase the learning rate by .01\n",
    "n_epochs = 100 # double number of epochs\n",
    "batch_size = 200 # double batch size\n",
    "n_batches = int(X_train.shape[0] / batch_size) # int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "# input images\n",
    "x = tf.placeholder(dtype=tf.float32, name=\"x\", shape=[None, num_inputs]) \n",
    "# target output\n",
    "y = tf.placeholder(dtype=tf.float32, name=\"y\", shape=[None, num_outputs]) \n",
    "\n",
    "model = mlp(x=x, \n",
    "            num_inputs=num_inputs, \n",
    "            num_outputs=num_outputs, \n",
    "            num_layers=num_layers, \n",
    "            num_neurons=num_neurons)\n",
    "\n",
    "# loss function\n",
    "#loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(model), axis=1))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, \n",
    "                                                              labels=y))\n",
    "# optimizer function\n",
    "# try with adam optimizer instead of cross entropy. accuracy was about 92.7% with cross ent\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss) \n",
    "\n",
    "predictions_check = tf.equal(tf.argmax(model,1), tf.argmax(y,1))\n",
    "accuracy_function = tf.reduce_mean(tf.cast(predictions_check, tf.float32))\n",
    "\n",
    "tensorflow_classification(n_epochs=n_epochs, \n",
    "                          n_batches=n_batches, \n",
    "                          batch_size=batch_size, \n",
    "                          batch_func=mnist_batch_func, \n",
    "                          model = model, \n",
    "                          optimizer = optimizer, \n",
    "                          loss = loss, \n",
    "                          accuracy_function = accuracy_function, \n",
    "                          X_test = X_test, \n",
    "                          Y_test = Y_test\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 3 layers (256 units/layer), our lowest loss came on epoch 31 (lr = .02). The ending accuracy was 10.2%. What happens if we decrease the learning rate by quite a bit and increase the number of epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0000   loss = 2056.097569\n",
      "epoch: 0001   loss = 478.991458\n",
      "epoch: 0002   loss = 304.959161\n",
      "epoch: 0003   loss = 216.539709\n",
      "epoch: 0004   loss = 161.929935\n",
      "epoch: 0005   loss = 123.638231\n",
      "epoch: 0006   loss = 95.015397\n",
      "epoch: 0007   loss = 72.046458\n",
      "epoch: 0008   loss = 56.457008\n",
      "epoch: 0009   loss = 44.051337\n",
      "epoch: 0010   loss = 34.979841\n",
      "epoch: 0011   loss = 27.496661\n",
      "epoch: 0012   loss = 21.678324\n",
      "epoch: 0013   loss = 16.429627\n",
      "epoch: 0014   loss = 13.029101\n",
      "epoch: 0015   loss = 11.156421\n",
      "epoch: 0016   loss = 8.302198\n",
      "epoch: 0017   loss = 6.901540\n",
      "epoch: 0018   loss = 5.998208\n",
      "epoch: 0019   loss = 4.994918\n",
      "epoch: 0020   loss = 4.897381\n",
      "epoch: 0021   loss = 4.194414\n",
      "epoch: 0022   loss = 3.944174\n",
      "epoch: 0023   loss = 3.652911\n",
      "epoch: 0024   loss = 3.284821\n",
      "epoch: 0025   loss = 2.817196\n",
      "epoch: 0026   loss = 3.837166\n",
      "epoch: 0027   loss = 2.893880\n",
      "epoch: 0028   loss = 2.996125\n",
      "epoch: 0029   loss = 2.790846\n",
      "epoch: 0030   loss = 2.456585\n",
      "epoch: 0031   loss = 3.045596\n",
      "epoch: 0032   loss = 2.923795\n",
      "epoch: 0033   loss = 2.946786\n",
      "epoch: 0034   loss = 2.271075\n",
      "epoch: 0035   loss = 2.008491\n",
      "epoch: 0036   loss = 2.058417\n",
      "epoch: 0037   loss = 2.712348\n",
      "epoch: 0038   loss = 2.521701\n",
      "epoch: 0039   loss = 1.764428\n",
      "epoch: 0040   loss = 2.410934\n",
      "epoch: 0041   loss = 1.832689\n",
      "epoch: 0042   loss = 2.187188\n",
      "epoch: 0043   loss = 1.901258\n",
      "epoch: 0044   loss = 1.956475\n",
      "epoch: 0045   loss = 2.626512\n",
      "epoch: 0046   loss = 2.035331\n",
      "epoch: 0047   loss = 1.543010\n",
      "epoch: 0048   loss = 1.180044\n",
      "epoch: 0049   loss = 1.615395\n",
      "epoch: 0050   loss = 1.142785\n",
      "epoch: 0051   loss = 1.602859\n",
      "epoch: 0052   loss = 1.537763\n",
      "epoch: 0053   loss = 1.974464\n",
      "epoch: 0054   loss = 1.744444\n",
      "epoch: 0055   loss = 1.567869\n",
      "epoch: 0056   loss = 1.364149\n",
      "epoch: 0057   loss = 1.421472\n",
      "epoch: 0058   loss = 1.466804\n",
      "epoch: 0059   loss = 1.138013\n",
      "epoch: 0060   loss = 1.232661\n",
      "epoch: 0061   loss = 1.932922\n",
      "epoch: 0062   loss = 1.918235\n",
      "epoch: 0063   loss = 1.257939\n",
      "epoch: 0064   loss = 1.328651\n",
      "epoch: 0065   loss = 1.765695\n",
      "epoch: 0066   loss = 1.199858\n",
      "epoch: 0067   loss = 0.547436\n",
      "epoch: 0068   loss = 2.714675\n",
      "epoch: 0069   loss = 1.321204\n",
      "epoch: 0070   loss = 0.891588\n",
      "epoch: 0071   loss = 1.447932\n",
      "epoch: 0072   loss = 0.552145\n",
      "epoch: 0073   loss = 1.046476\n",
      "epoch: 0074   loss = 1.702633\n",
      "epoch: 0075   loss = 0.765100\n",
      "epoch: 0076   loss = 1.043576\n",
      "epoch: 0077   loss = 1.292115\n",
      "epoch: 0078   loss = 1.757641\n",
      "epoch: 0079   loss = 1.495233\n",
      "epoch: 0080   loss = 1.025084\n",
      "epoch: 0081   loss = 1.262239\n",
      "epoch: 0082   loss = 0.811503\n",
      "epoch: 0083   loss = 1.384282\n",
      "epoch: 0084   loss = 1.393755\n",
      "epoch: 0085   loss = 0.924077\n",
      "epoch: 0086   loss = 1.249832\n",
      "epoch: 0087   loss = 1.037325\n",
      "epoch: 0088   loss = 1.062154\n",
      "epoch: 0089   loss = 0.870053\n",
      "epoch: 0090   loss = 0.803987\n",
      "epoch: 0091   loss = 0.856849\n",
      "epoch: 0092   loss = 0.626024\n",
      "epoch: 0093   loss = 0.747401\n",
      "epoch: 0094   loss = 1.179917\n",
      "epoch: 0095   loss = 0.879346\n",
      "epoch: 0096   loss = 1.153840\n",
      "epoch: 0097   loss = 1.280827\n",
      "epoch: 0098   loss = 1.161046\n",
      "epoch: 0099   loss = 0.609281\n",
      "accuracy=0.96280003\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "num_layers = 3 \n",
    "num_neurons = []\n",
    "for i in range(num_layers):\n",
    "    num_neurons.append(256)\n",
    "    \n",
    "learning_rate = 0.001 # decrease learning rate\n",
    "n_epochs = 100 # double number of epochs\n",
    "batch_size = 200 # double batch size\n",
    "n_batches = int(X_train.shape[0] / batch_size) # int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "# input images\n",
    "x = tf.placeholder(dtype=tf.float32, name=\"x\", shape=[None, num_inputs]) \n",
    "# target output\n",
    "y = tf.placeholder(dtype=tf.float32, name=\"y\", shape=[None, num_outputs]) \n",
    "\n",
    "model = mlp(x=x, \n",
    "            num_inputs=num_inputs, \n",
    "            num_outputs=num_outputs, \n",
    "            num_layers=num_layers, \n",
    "            num_neurons=num_neurons)\n",
    "\n",
    "# loss function\n",
    "#loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(model), axis=1))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, \n",
    "                                                              labels=y))\n",
    "# optimizer function\n",
    "# try with adam optimizer instead of cross entropy. accuracy was about 92.7% with cross ent\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss) \n",
    "\n",
    "predictions_check = tf.equal(tf.argmax(model,1), tf.argmax(y,1))\n",
    "accuracy_function = tf.reduce_mean(tf.cast(predictions_check, tf.float32))\n",
    "\n",
    "tensorflow_classification(n_epochs=n_epochs, \n",
    "                          n_batches=n_batches, \n",
    "                          batch_size=batch_size, \n",
    "                          batch_func=mnist_batch_func, \n",
    "                          model = model, \n",
    "                          optimizer = optimizer, \n",
    "                          loss = loss, \n",
    "                          accuracy_function = accuracy_function, \n",
    "                          X_test = X_test, \n",
    "                          Y_test = Y_test\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get an accuracy of about 97%, which is an improvement over the 2 layer architecture. It seems that hyperparameters will need to be highly tuned as the model can be very fragile depending on the architecture and parameters. This probably isn't the best out of the box model! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP in Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use parameters from 3 layer architecture as it seemed to perform slightly better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 335,114\n",
      "Trainable params: 335,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.2825 - acc: 0.9195\n",
      "Epoch 2/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0986 - acc: 0.9698\n",
      "Epoch 3/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0654 - acc: 0.9798\n",
      "Epoch 4/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0461 - acc: 0.9855\n",
      "Epoch 5/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0347 - acc: 0.9888\n",
      "Epoch 6/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0289 - acc: 0.9907\n",
      "Epoch 7/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0216 - acc: 0.9930\n",
      "Epoch 8/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0179 - acc: 0.9943\n",
      "Epoch 9/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0169 - acc: 0.9943\n",
      "Epoch 10/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0189 - acc: 0.9936\n",
      "Epoch 11/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0131 - acc: 0.9955\n",
      "Epoch 12/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0136 - acc: 0.9957\n",
      "Epoch 13/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0082 - acc: 0.9972\n",
      "Epoch 14/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0150 - acc: 0.9950\n",
      "Epoch 15/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0120 - acc: 0.9958\n",
      "Epoch 16/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0129 - acc: 0.9956\n",
      "Epoch 17/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0082 - acc: 0.9973\n",
      "Epoch 18/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0072 - acc: 0.9978\n",
      "Epoch 19/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0077 - acc: 0.9974\n",
      "Epoch 20/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0121 - acc: 0.9958\n",
      "Epoch 21/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0098 - acc: 0.9970\n",
      "Epoch 22/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0078 - acc: 0.9975\n",
      "Epoch 23/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0100 - acc: 0.9969\n",
      "Epoch 24/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0063 - acc: 0.9980\n",
      "Epoch 25/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0072 - acc: 0.9978\n",
      "Epoch 26/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0055 - acc: 0.9983\n",
      "Epoch 27/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0054 - acc: 0.9984\n",
      "Epoch 28/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0082 - acc: 0.9972\n",
      "Epoch 29/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0041 - acc: 0.9988\n",
      "Epoch 30/70\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0084 - acc: 0.9975\n",
      "Epoch 31/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0080 - acc: 0.9974\n",
      "Epoch 32/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0074 - acc: 0.9978\n",
      "Epoch 33/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0054 - acc: 0.9984\n",
      "Epoch 34/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0041 - acc: 0.9986\n",
      "Epoch 35/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0048 - acc: 0.9986\n",
      "Epoch 36/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0037 - acc: 0.9987\n",
      "Epoch 37/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0068 - acc: 0.9978\n",
      "Epoch 38/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0046 - acc: 0.9985\n",
      "Epoch 39/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0029 - acc: 0.9989\n",
      "Epoch 40/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0052 - acc: 0.9984\n",
      "Epoch 41/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0060 - acc: 0.9984\n",
      "Epoch 42/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0064 - acc: 0.9983\n",
      "Epoch 43/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0028 - acc: 0.9991\n",
      "Epoch 44/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0043 - acc: 0.9986\n",
      "Epoch 45/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 46/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0070 - acc: 0.9982\n",
      "Epoch 47/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0084 - acc: 0.9973\n",
      "Epoch 48/70\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.0030 - acc: 0.9990\n",
      "Epoch 49/70\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0014 - acc: 0.9996\n",
      "Epoch 50/70\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.0030 - acc: 0.9991\n",
      "Epoch 51/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0034 - acc: 0.9991\n",
      "Epoch 52/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0064 - acc: 0.9981\n",
      "Epoch 53/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0066 - acc: 0.9980\n",
      "Epoch 54/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 55/70\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 7.0862e-04 - acc: 0.9998\n",
      "Epoch 56/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0021 - acc: 0.9994\n",
      "Epoch 57/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0086 - acc: 0.9976\n",
      "Epoch 58/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0037 - acc: 0.9991\n",
      "Epoch 59/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0023 - acc: 0.9993\n",
      "Epoch 60/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0051 - acc: 0.9986\n",
      "Epoch 61/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0043 - acc: 0.9987\n",
      "Epoch 62/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0029 - acc: 0.9992\n",
      "Epoch 63/70\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 64/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0041 - acc: 0.9988\n",
      "Epoch 65/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 66/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0047 - acc: 0.9988\n",
      "Epoch 67/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0031 - acc: 0.9992\n",
      "Epoch 68/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0030 - acc: 0.9991\n",
      "Epoch 69/70\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.0010 - acc: 0.9997\n",
      "Epoch 70/70\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.0014 - acc: 0.9996\n",
      "10000/10000 [==============================] - 0s 42us/step\n",
      "\n",
      "Test loss: 0.11772566685567233\n",
      "Test accuracy: 0.9832\n"
     ]
    }
   ],
   "source": [
    "num_layers = 3\n",
    "num_neurons = []\n",
    "for i in range(num_layers):\n",
    "    num_neurons.append(256)\n",
    "    \n",
    "learning_rate = 0.001\n",
    "n_epochs = 70\n",
    "batch_size = 200\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=num_neurons[0], activation='relu', \n",
    "                input_shape=(num_inputs,)))\n",
    "model.add(Dense(units=num_neurons[1], activation='relu'))\n",
    "model.add(Dense(units=num_neurons[2], activation='relu'))\n",
    "model.add(Dense(units=num_outputs, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=learning_rate),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=n_epochs)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test)\n",
    "print('\\nTest loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took the model about 45 epochs to figure out the data entirely. Interestingly, It still generalizes pretty well to the data with an accuracy of 98.5%. How does Keras go about picking the model after it trains through many epochs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing CPU vs GPU performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 14899481088090155448, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 3186409472\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 6510960725780339037\n",
       " physical_device_desc: \"device: 0, name: Quadro M3000M, pci bus id: 0000:01:00.0, compute capability: 5.2\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from tensorflow.python.client import device_lib\n",
    "import time\n",
    "\n",
    "# list local devices\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a list of two sessions. one gpu and one cpu\n",
    "session_conf = [tf.ConfigProto(device_count = {'CPU' : 1, 'GPU' : 0}),\n",
    "                tf.ConfigProto(device_count = {'CPU' : 1, 'GPU' : 1})]\n",
    "session_names = [\"CPU\", \"GPU\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model runtime on CPU: 158.7375466823578\n",
      "\n",
      "10000/10000 [==============================] - 0s 36us/step\n",
      "CPU Test loss:\n",
      " 0.11355301066923307\n",
      "CPU Test accuracy:\n",
      "\n",
      " 0.9808\n",
      "Model runtime on GPU: 102.61378335952759\n",
      "\n",
      "10000/10000 [==============================] - 0s 42us/step\n",
      "GPU Test loss:\n",
      " 0.09766645804248444\n",
      "GPU Test accuracy:\n",
      "\n",
      " 0.986\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(session_conf)):\n",
    "    sess = tf.Session(graph=tf.get_default_graph(),\n",
    "                      config=session_conf[i])\n",
    "    K.set_session(sess)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=num_neurons[0], activation='relu', \n",
    "                    input_shape=(num_inputs,)))\n",
    "    model.add(Dense(units=num_neurons[1], activation='relu'))\n",
    "    model.add(Dense(units=num_neurons[2], activation='relu'))\n",
    "    model.add(Dense(units=num_outputs, activation='softmax'))\n",
    "    # model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    t1 = time.time()\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=n_epochs,\n",
    "              verbose=0)\n",
    "    t2 = time.time()\n",
    "    print(\"Model runtime on {}: {}\\n\".format(session_names[i], t2 - t1))\n",
    "    \n",
    "    score = model.evaluate(X_test, Y_test)\n",
    "    print('{} Test loss:\\n'.format(session_names[i]), score[0])\n",
    "    print('{} Test accuracy:\\n\\n'.format(session_names[i]), score[1])\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP in TFLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 29999  | total loss: \u001b[1m\u001b[32m0.09842\u001b[0m\u001b[0m | time: 3.379s\n",
      "| SGD | epoch: 050 | loss: 0.09842 - acc: 0.9701 -- iter: 59900/60000\n",
      "Training Step: 30000  | total loss: \u001b[1m\u001b[32m0.09391\u001b[0m\u001b[0m | time: 3.385s\n",
      "| SGD | epoch: 050 | loss: 0.09391 - acc: 0.9721 -- iter: 60000/60000\n",
      "--\n",
      "Test accuracy: 0.9681\n"
     ]
    }
   ],
   "source": [
    "num_layers = 2\n",
    "num_neurons = []\n",
    "for i in range(num_layers):\n",
    "    num_neurons.append(256)\n",
    "    \n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "# Build deep neural network\n",
    "input_layer = tflearn.input_data(shape=[None, num_inputs])\n",
    "dense1 = tflearn.fully_connected(input_layer, num_neurons[0], activation='relu')\n",
    "dense2 = tflearn.fully_connected(dense1, num_neurons[1], activation='relu')\n",
    "softmax = tflearn.fully_connected(dense2, num_outputs, activation='softmax')\n",
    "\n",
    "optimizer = tflearn.SGD(learning_rate=learning_rate)\n",
    "net = tflearn.regression(softmax, optimizer=optimizer, \n",
    "                         metric=tflearn.metrics.Accuracy(), \n",
    "                         loss='categorical_crossentropy')\n",
    "model = tflearn.DNN(net)\n",
    "\n",
    "model.fit(X_train, Y_train, \n",
    "          n_epoch=n_epochs, batch_size=batch_size, \n",
    "          show_metric=True, run_id='dense_model')\n",
    "\n",
    "score = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', score[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeSeries Data - MLP - Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# dataframe = pd.read_csv(os.path.join(datasetslib.datasets_root, \n",
    "#                                      'ts-data', \n",
    "#                                      'international-airline-passengers-cleaned.csv'), \n",
    "#                         usecols=[1],header=0)\n",
    "\n",
    "dataset = pd.read_csv(\"international-airline-passengers.csv\")\n",
    "dataset.columns = ['month', 'passengers']\n",
    "dataset.set_index(['month'], inplace=True)\n",
    "# dataset = dataframe.values\n",
    "# dataset = dataset.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passengers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1949-01</th>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949-02</th>\n",
       "      <td>118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949-03</th>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949-04</th>\n",
       "      <td>129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949-05</th>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         passengers\n",
       "month              \n",
       "1949-01       112.0\n",
       "1949-02       118.0\n",
       "1949-03       132.0\n",
       "1949-04       129.0\n",
       "1949-05       121.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# dataset.passengers = scaler.fit_transform(dataset.passengers.values.reshape((-1, 1)))\n",
    "\n",
    "#scaler = skpp.MinMaxScaler(feature_range=(0, 1))\n",
    "#normalized_dataset = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passengers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1949-01</th>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949-02</th>\n",
       "      <td>118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949-03</th>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949-04</th>\n",
       "      <td>129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949-05</th>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         passengers\n",
       "month              \n",
       "1949-01       112.0\n",
       "1949-02       118.0\n",
       "1949-03       132.0\n",
       "1949-04       129.0\n",
       "1949-05       121.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 48\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "# train,test=dsu.train_test_split(dataset,train_size=0.67)\n",
    "    \n",
    "train, test = train_test_split(dataset, test_size=.33, shuffle=False)\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into X=t-1,t and Y=t+1\n",
    "n_x=2\n",
    "n_y=1\n",
    "# X_train, Y_train, X_test, Y_test = dsu.mvts_to_xy(train,test,n_x=n_x,n_y=n_y)\n",
    "\n",
    "# train\n",
    "X_train = pd.concat([train.shift(1), train], axis=1).dropna(axis=0).iloc[:-1, :]\n",
    "X_train.columns = ['passengers_lag_1', 'passengers']\n",
    "\n",
    "Y_train = train.shift(-1).iloc[1:, :].dropna(axis=0)\n",
    "\n",
    "# test\n",
    "X_test = pd.concat([test.shift(1), test], axis=1).dropna(axis=0).iloc[:-1, :]\n",
    "X_test.columns = ['passengers_lag_1', 'passengers']\n",
    "\n",
    "Y_test = test.shift(-1).iloc[1:, :].dropna(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      "95\n",
      "45\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(Y_train))\n",
    "print(len(X_test))\n",
    "print(len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         passengers\n",
      "month              \n",
      "1949-01       112.0\n",
      "1949-02       118.0\n",
      "1949-03       132.0\n",
      "1949-04       129.0\n",
      "1949-05       121.0\n",
      "         passengers\n",
      "month              \n",
      "1956-09       355.0\n",
      "1956-10       306.0\n",
      "1956-11       271.0\n",
      "1956-12       306.0\n",
      "1957-01       315.0\n"
     ]
    }
   ],
   "source": [
    "print(train.head())\n",
    "print(train.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         passengers_lag_1  passengers\n",
      "month                                \n",
      "1949-02             112.0       118.0\n",
      "1949-03             118.0       132.0\n",
      "1949-04             132.0       129.0\n",
      "1949-05             129.0       121.0\n",
      "1949-06             121.0       135.0\n",
      "         passengers\n",
      "month              \n",
      "1949-02       132.0\n",
      "1949-03       129.0\n",
      "1949-04       121.0\n",
      "1949-05       135.0\n",
      "1949-06       148.0\n",
      "         passengers_lag_1  passengers\n",
      "month                                \n",
      "1956-08             413.0       405.0\n",
      "1956-09             405.0       355.0\n",
      "1956-10             355.0       306.0\n",
      "1956-11             306.0       271.0\n",
      "1956-12             271.0       306.0\n",
      "         passengers\n",
      "month              \n",
      "1956-08       355.0\n",
      "1956-09       306.0\n",
      "1956-10       271.0\n",
      "1956-11       306.0\n",
      "1956-12       315.0\n"
     ]
    }
   ],
   "source": [
    "print(X_train.head())\n",
    "print(Y_train.head())\n",
    "print(X_train.tail())\n",
    "print(Y_train.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95, 2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8)                 24        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 105\n",
      "Trainable params: 105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "95/95 [==============================] - 0s 4ms/step - loss: 56813.1999\n",
      "Epoch 2/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 33177.3972\n",
      "Epoch 3/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 18112.4463\n",
      "Epoch 4/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 8874.3510\n",
      "Epoch 5/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 3697.9450\n",
      "Epoch 6/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 1364.8877\n",
      "Epoch 7/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 661.9248\n",
      "Epoch 8/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 552.0576\n",
      "Epoch 9/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 536.7808\n",
      "Epoch 10/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 544.2915\n",
      "Epoch 11/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 536.9509\n",
      "Epoch 12/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 546.0723\n",
      "Epoch 13/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 539.7399\n",
      "Epoch 14/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 539.8857\n",
      "Epoch 15/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 536.8595\n",
      "Epoch 16/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 539.0551\n",
      "Epoch 17/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 538.2627\n",
      "Epoch 18/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 533.6266\n",
      "Epoch 19/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 537.9909\n",
      "Epoch 20/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 541.7278\n",
      "Epoch 21/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 542.6058\n",
      "Epoch 22/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 534.8311\n",
      "Epoch 23/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 535.1377\n",
      "Epoch 24/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 545.8858\n",
      "Epoch 25/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 531.4217\n",
      "Epoch 26/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 537.5706\n",
      "Epoch 27/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 544.4230\n",
      "Epoch 28/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 532.6096\n",
      "Epoch 29/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 546.8167\n",
      "Epoch 30/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 547.2383\n",
      "Epoch 31/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 542.1111\n",
      "Epoch 32/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 537.2249\n",
      "Epoch 33/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 530.5665\n",
      "Epoch 34/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 530.7292\n",
      "Epoch 35/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 537.9725\n",
      "Epoch 36/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 543.9256\n",
      "Epoch 37/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 528.0194\n",
      "Epoch 38/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 534.7354\n",
      "Epoch 39/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 530.6569\n",
      "Epoch 40/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 535.0095\n",
      "Epoch 41/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 529.1995\n",
      "Epoch 42/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 537.2150\n",
      "Epoch 43/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 541.0319\n",
      "Epoch 44/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 539.1988\n",
      "Epoch 45/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 535.8034\n",
      "Epoch 46/50\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 553.9254\n",
      "Epoch 47/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 542.4863\n",
      "Epoch 48/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 529.4994\n",
      "Epoch 49/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 530.3912\n",
      "Epoch 50/50\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 542.3681\n",
      "45/45 [==============================] - 0s 643us/step\n",
      "\n",
      "Test mse: 2251.2456434461806\n",
      "Test rmse: 47.44729332054865\n"
     ]
    }
   ],
   "source": [
    "num_layers = 2\n",
    "num_neurons = [8,8]\n",
    "n_epochs = 50\n",
    "batch_size = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(num_neurons[0], activation='relu', input_shape=(n_x,)))\n",
    "model.add(Dense(num_neurons[1], activation='relu'))\n",
    "model.add(Dense(units=1))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "model.fit(X_train.values, Y_train.values,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=n_epochs)\n",
    "\n",
    "score = model.evaluate(X_test.values, Y_test.values)\n",
    "print('\\nTest mse:', score)\n",
    "print('Test rmse:', math.sqrt(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "Y_train_pred = model.predict(X_train)\n",
    "Y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift train predictions for plotting\n",
    "Y_train_pred_plot = np.empty_like(dataset)\n",
    "Y_train_pred_plot[:, :] = np.nan\n",
    "Y_train_pred_plot[n_x-1:len(Y_train_pred)+n_x-1, :] = Y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift test predictions for plotting\n",
    "Y_test_pred_plot = np.empty_like(dataset)\n",
    "Y_test_pred_plot[:, :] = np.nan\n",
    "Y_test_pred_plot[len(Y_train_pred)+(n_x*2)-1:len(dataset)-2, :] = Y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot baseline and predictions\n",
    "plt.plot(dataset.values,label='Original Data')\n",
    "plt.plot(Y_train_pred_plot,label='Y_train_pred')\n",
    "plt.plot(Y_test_pred_plot,label='Y_test_pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
